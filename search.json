[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Oxinfer",
    "section": "",
    "text": "Please be aware some of the sections of this website are still under construction\nPlease if you detect inconsistencies or incompletness please open issues: https://github.com/oxford-pharmacoepi/OxinferOnboarding"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Oxinfer",
    "section": "Introduction",
    "text": "Introduction\nThis website covers the basis to know what is the Oxinfer group working on and which are the crucial concepts one must know when you start to work with us.\nThis book starts with an overall description of the team, its aim and scope, and its members in ?@sec-team. It is particularly interesting to know about the members of our team and how to work with them.\nThe next 3 chapters provide the basis of our work:\n\nEpidemiology (?@sec-epi).\nThe OMOP Common Data Model (?@sec-omop).\nR (?@sec-R)\n\nThese 3 chapters cover resources and basic knowledge that you can skip if you are familiar with.\nFinally, the last chapter (?@sec-checklist) provides a checklist of all the steps that you need to follow to be onboarded to the group."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Oxinfer",
    "section": "License",
    "text": "License\n This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "onboarding/github.html",
    "href": "onboarding/github.html",
    "title": "Github",
    "section": "",
    "text": "Git is a powerful tool that helps developers manage and track changes in their projects. It allows multiple people to work on the same project simultaneously without causing conflicts or losing any work. Git keeps a record of every change made, which makes it easy to go back to previous versions if needed. This makes collaboration smoother and ensures that the project evolves in an organized and reliable way.\nGitHub is a web platform built on top of Git where developers can host their projects and collaborate more effectively. It provides a space where people can share their code, discuss issues, and plan their work.\nGithub desktop is a user-friendly way to use git and github interface from an app. You can download it here: https://github.com/apps/desktop. We usually use github desktop to perform commits, push, pull and perform the different github operations. If you are very skilled you can also use git from the terminal.\nIf you are interested to know more you can read it here, you can follow this tutorial.\n\n\n\nWe will use github for three different purposes:\n\nDevelop study code in a collaborative way. Multiple users will contribute to the same repo. This repositories will be usually kept as private so we will have to give them access to the individuals that can edit it.\nShare study code for a network study.\nDevelop an R package in a collaborative way.\n\nIf you don’t have a github account you can create one at https://github.com\n\n\n\n\n\nA repository, often shortened to “repo” is a storage space where your project lives. It can contain files, folders, images, videos, spreadsheets, data sets, and anything else your project needs. A GitHub repository also includes a history of all changes made to the files within it, making it easy to track and manage your project’s development.\n\n\n\nCloning a repository means creating a local copy of a project that is stored on GitHub. This allows you to work on the project on your own machine. To clone a repository, you can use the following steps (Github desktop):\n\nTop left: Add -&gt; Clone Repository…\nprovide user or organization name and the name of the repo that you want to clone separated by “/”. Example: ‘oxford-pharmacoepi/OxinferOnboarding’.\nClone\n\nIf your repo contains an R project you can open it from RStidio and edit there what you want.\n\n\n\nA branch in GitHub is a parallel version of a repository. It diverges from the main working project to work on something without affecting the main branch. Branches are used to develop features, fix bugs, or experiment safely. For example it is useful if you are working collaboratively to make some changes to a repository without affecting the others and submitting your changes so someone can review them.\nIn general if the repo is for your use only you will always work on the “main” branch. You can switch or create a new branch using the branch tab in Github desktop.\n\n\n\nA commit is a snapshot of your repository at a specific point in time. When you make changes to files in your repository, you can “commit” those changes, which records them in the repository’s history. Each commit has a unique identifier (a hash), a message describing the changes, and metadata like the author’s name and the timestamp.\nTo create a commit you just have to tick the files that you want to commit, add a message and click commit to create the commit.\nInitially this commit will be only local, you could send it online using push. Then all your changes have been uploaded to the github platform.\n\n\n\n\nPush: When you push changes, you send your committed changes from your local repository to a remote repository on GitHub. This makes your changes available to others.\nPull: Pulling is the process of fetching updates from a remote repository and merging them into your local repository. This ensures your local repository is up to date with the latest changes made by others.\n\n\n\n\nA pull request (PR) is a way to propose changes to a repository. After you’ve made changes in a branch (not main), you can open a pull request to merge those changes into another branch, typically the main branch. Other team members can review, comment, and approve the changes before they are merged.\nBy understanding these fundamental concepts, you can effectively use GitHub to manage your projects and collaborate with others.\n\n\n\nIssues on GitHub are used to track tasks, enhancements, and bugs for your projects. They are a great way to organize and prioritize your work. Each issue can include a title, a detailed description, labels, and comments from collaborators.\nTo create an issue:\n\nGo to the repository on GitHub.\nClick the “Issues” tab.\nClick the “New issue” button.\nFill in the title and description, and submit the issue.\n\nIssues can be assigned to team members, labeled with tags, and linked to pull requests, making them a powerful tool for project management.\n\n\n\nThe .gitignore file is used to specify files and directories that Git should ignore. This is useful for excluding files that are not meant to be tracked, such as temporary files, build outputs, and sensitive information.\nTo create and use a .gitignore file:\n\nCreate a file named .gitignore in the root of your repository.\nAdd the files and directories you want to ignore. For example:\n\n\n# Ignore all .csv files\n*.csv\n   \n# Ignore the Results directory\nResults/\n   \n# Ignore sensitive files (in this case)\nconnectToDatabase.R\n\nBy using a .gitignore file, you can keep your repository clean and free of unnecessary files, ensuring that only relevant files are tracked and shared.\n\n\n\n\nYou can configure your github account in your RStudio session so it is easier to use it. This is mainly recommended when you can not use github desktop (e.g. on the server).\nYou can learn how to configure git and github in Rstudio following this tutorial.\n\n\nIn general you can install an R package from github using devtools or remotes package:\n\nremotes::install_github(\"darwin-eu/CDMConnector\")\n\nSometimes we keep the development of some packages private, so we can include mentions of specific studies or keep the issues private. For example there exist a private version of CDMConnector in darwin-eu-dev, if you try to install it you would get the following error:\n\nremotes::install_github(\"darwin-eu-dev/CDMConnector\")\n\nUsing github PAT from envvar GITHUB_TOKEN. Use `gitcreds::gitcreds_set()` and unset GITHUB_TOKEN in .Renviron (or elsewhere) if you want to use the more secure git credential store instead.\n\n\nError: Failed to install 'CDMConnector' from GitHub:\n  HTTP error 404.\n  Not Found\n\n  Did you spell the repo owner (`darwin-eu-dev`) and repo name (`CDMConnector`) correctly?\n  - If spelling is correct, check that you have the required permissions to access the repo.\n\n\nSo you can install private packages (that you have access to) you need to set up the GITHUB_PAT. You can set it up with the following steps:\n\nLog in to github and go to the tokens section: https://github.com/settings/tokens.\nSelect the option: Generate new token (classic) (it is in the top-right dropdown menu Generate new token -&gt; Generate new token (classic)).\nAdd a note so you know what is this token for and tick at least the following box: repo; and click on Generate token.\nCopy the token (it should be a combination of letters and numbers that starts with ghp_...).\nSet a new .Renviron variable called GITHUB_PAT.\n\n\nusethis::edit_r_environ()\n# write there\nGITHUB_PAT = \"ghp_...\"\n\n\nRestart your R session.\n\nNow you should be able to install the private repo (if you have access to it):\n\nremotes::install_github(\"darwin-eu-dev/CDMConnector\")\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is very important that we do not commit to github: - personal information - study results - database credentials\nKeep in mind that after you push some changes in a github repo, even if you delete them later those changes will remain in the commit history.",
    "crumbs": [
      "Onboarding",
      "Github"
    ]
  },
  {
    "objectID": "onboarding/github.html#git-github-and-github-desktop",
    "href": "onboarding/github.html#git-github-and-github-desktop",
    "title": "Github",
    "section": "",
    "text": "Git is a powerful tool that helps developers manage and track changes in their projects. It allows multiple people to work on the same project simultaneously without causing conflicts or losing any work. Git keeps a record of every change made, which makes it easy to go back to previous versions if needed. This makes collaboration smoother and ensures that the project evolves in an organized and reliable way.\nGitHub is a web platform built on top of Git where developers can host their projects and collaborate more effectively. It provides a space where people can share their code, discuss issues, and plan their work.\nGithub desktop is a user-friendly way to use git and github interface from an app. You can download it here: https://github.com/apps/desktop. We usually use github desktop to perform commits, push, pull and perform the different github operations. If you are very skilled you can also use git from the terminal.\nIf you are interested to know more you can read it here, you can follow this tutorial.",
    "crumbs": [
      "Onboarding",
      "Github"
    ]
  },
  {
    "objectID": "onboarding/github.html#use",
    "href": "onboarding/github.html#use",
    "title": "Github",
    "section": "",
    "text": "We will use github for three different purposes:\n\nDevelop study code in a collaborative way. Multiple users will contribute to the same repo. This repositories will be usually kept as private so we will have to give them access to the individuals that can edit it.\nShare study code for a network study.\nDevelop an R package in a collaborative way.\n\nIf you don’t have a github account you can create one at https://github.com",
    "crumbs": [
      "Onboarding",
      "Github"
    ]
  },
  {
    "objectID": "onboarding/github.html#the-basis",
    "href": "onboarding/github.html#the-basis",
    "title": "Github",
    "section": "",
    "text": "A repository, often shortened to “repo” is a storage space where your project lives. It can contain files, folders, images, videos, spreadsheets, data sets, and anything else your project needs. A GitHub repository also includes a history of all changes made to the files within it, making it easy to track and manage your project’s development.\n\n\n\nCloning a repository means creating a local copy of a project that is stored on GitHub. This allows you to work on the project on your own machine. To clone a repository, you can use the following steps (Github desktop):\n\nTop left: Add -&gt; Clone Repository…\nprovide user or organization name and the name of the repo that you want to clone separated by “/”. Example: ‘oxford-pharmacoepi/OxinferOnboarding’.\nClone\n\nIf your repo contains an R project you can open it from RStidio and edit there what you want.\n\n\n\nA branch in GitHub is a parallel version of a repository. It diverges from the main working project to work on something without affecting the main branch. Branches are used to develop features, fix bugs, or experiment safely. For example it is useful if you are working collaboratively to make some changes to a repository without affecting the others and submitting your changes so someone can review them.\nIn general if the repo is for your use only you will always work on the “main” branch. You can switch or create a new branch using the branch tab in Github desktop.\n\n\n\nA commit is a snapshot of your repository at a specific point in time. When you make changes to files in your repository, you can “commit” those changes, which records them in the repository’s history. Each commit has a unique identifier (a hash), a message describing the changes, and metadata like the author’s name and the timestamp.\nTo create a commit you just have to tick the files that you want to commit, add a message and click commit to create the commit.\nInitially this commit will be only local, you could send it online using push. Then all your changes have been uploaded to the github platform.\n\n\n\n\nPush: When you push changes, you send your committed changes from your local repository to a remote repository on GitHub. This makes your changes available to others.\nPull: Pulling is the process of fetching updates from a remote repository and merging them into your local repository. This ensures your local repository is up to date with the latest changes made by others.\n\n\n\n\nA pull request (PR) is a way to propose changes to a repository. After you’ve made changes in a branch (not main), you can open a pull request to merge those changes into another branch, typically the main branch. Other team members can review, comment, and approve the changes before they are merged.\nBy understanding these fundamental concepts, you can effectively use GitHub to manage your projects and collaborate with others.\n\n\n\nIssues on GitHub are used to track tasks, enhancements, and bugs for your projects. They are a great way to organize and prioritize your work. Each issue can include a title, a detailed description, labels, and comments from collaborators.\nTo create an issue:\n\nGo to the repository on GitHub.\nClick the “Issues” tab.\nClick the “New issue” button.\nFill in the title and description, and submit the issue.\n\nIssues can be assigned to team members, labeled with tags, and linked to pull requests, making them a powerful tool for project management.\n\n\n\nThe .gitignore file is used to specify files and directories that Git should ignore. This is useful for excluding files that are not meant to be tracked, such as temporary files, build outputs, and sensitive information.\nTo create and use a .gitignore file:\n\nCreate a file named .gitignore in the root of your repository.\nAdd the files and directories you want to ignore. For example:\n\n\n# Ignore all .csv files\n*.csv\n   \n# Ignore the Results directory\nResults/\n   \n# Ignore sensitive files (in this case)\nconnectToDatabase.R\n\nBy using a .gitignore file, you can keep your repository clean and free of unnecessary files, ensuring that only relevant files are tracked and shared.",
    "crumbs": [
      "Onboarding",
      "Github"
    ]
  },
  {
    "objectID": "onboarding/github.html#github-in-rstudio",
    "href": "onboarding/github.html#github-in-rstudio",
    "title": "Github",
    "section": "",
    "text": "You can configure your github account in your RStudio session so it is easier to use it. This is mainly recommended when you can not use github desktop (e.g. on the server).\nYou can learn how to configure git and github in Rstudio following this tutorial.\n\n\nIn general you can install an R package from github using devtools or remotes package:\n\nremotes::install_github(\"darwin-eu/CDMConnector\")\n\nSometimes we keep the development of some packages private, so we can include mentions of specific studies or keep the issues private. For example there exist a private version of CDMConnector in darwin-eu-dev, if you try to install it you would get the following error:\n\nremotes::install_github(\"darwin-eu-dev/CDMConnector\")\n\nUsing github PAT from envvar GITHUB_TOKEN. Use `gitcreds::gitcreds_set()` and unset GITHUB_TOKEN in .Renviron (or elsewhere) if you want to use the more secure git credential store instead.\n\n\nError: Failed to install 'CDMConnector' from GitHub:\n  HTTP error 404.\n  Not Found\n\n  Did you spell the repo owner (`darwin-eu-dev`) and repo name (`CDMConnector`) correctly?\n  - If spelling is correct, check that you have the required permissions to access the repo.\n\n\nSo you can install private packages (that you have access to) you need to set up the GITHUB_PAT. You can set it up with the following steps:\n\nLog in to github and go to the tokens section: https://github.com/settings/tokens.\nSelect the option: Generate new token (classic) (it is in the top-right dropdown menu Generate new token -&gt; Generate new token (classic)).\nAdd a note so you know what is this token for and tick at least the following box: repo; and click on Generate token.\nCopy the token (it should be a combination of letters and numbers that starts with ghp_...).\nSet a new .Renviron variable called GITHUB_PAT.\n\n\nusethis::edit_r_environ()\n# write there\nGITHUB_PAT = \"ghp_...\"\n\n\nRestart your R session.\n\nNow you should be able to install the private repo (if you have access to it):\n\nremotes::install_github(\"darwin-eu-dev/CDMConnector\")",
    "crumbs": [
      "Onboarding",
      "Github"
    ]
  },
  {
    "objectID": "onboarding/github.html#final-remarks",
    "href": "onboarding/github.html#final-remarks",
    "title": "Github",
    "section": "",
    "text": "Important\n\n\n\nIt is very important that we do not commit to github: - personal information - study results - database credentials\nKeep in mind that after you push some changes in a github repo, even if you delete them later those changes will remain in the commit history.",
    "crumbs": [
      "Onboarding",
      "Github"
    ]
  },
  {
    "objectID": "onboarding/useful_terms.html",
    "href": "onboarding/useful_terms.html",
    "title": "Useful terms and terminologies",
    "section": "",
    "text": "Useful terms and terminologies\nSometimes we can use words interchangeably which can cause confusion so below are some terms we use a lot in the team and what they mean and synonyms.\n\nNDORMS (Nuffield Department of Orthopaedics, Rheumatology and Musculoskeletal Sciences).\n\nThis is our department. NDORMS is part of the Medical Sciences Division of the University of Oxford.\n\nBotnar (Botnar Institute for Musculoskeletal Sciences)\n\nThis is where you work (if you dont why are you reading this?!). The research work in the department (NDORMS) takes place across three world-leading research institutes: the Botnar Institute for Musculoskeletal Sciences, the Kennedy Institute for Rheumatology and the Kadoorie Centre.\n\nHDS (Health Data Sciences)\n\nThe Botnar Institute is structured into four academic divisions. From the divisions there are sections. We work under academic section Health Data Sciences.\n\n\n\nBotnar Divisions\n\n\n\nCSM (Centre for Statistics in Medicine)\n\nThe CSM is based at Botnar with an aim to advance healthcare practice and policy that changes lives, through excellent research. Our group is one of the teams as well as others apart of the CSM.\n\nCDM (Common Data Model)\n\nA common data model is a standard that defines a common language. If people use this common language for their databases this means data can be harmonized. Think of it like a foreign plug adaptor, which turns data into one format. What this means is all databases are in the same format we can run the same code without writing bespoke code for each database we want to use for a study.\n\nOMOP (Observational Medical Outcomes Partnership)\n\nOMOP CDM is an open community data standard, designed to standardize the structure and content of observational data and to enable efficient analyses that can produce reliable evidence. In simple terms it is a type of common data model people could use.\n\nOHDSI (Observational Health Data Sciences and Informatics)\n\nThis is a program which is a global collaborative program to bring out the value of health data through large-scale analytics. Includes academia and industry. People in this collaboration use the OMOP CDM. More information here. Note: We lead the OHDSI UK hub.\n\nData partner\n\nThis is the term we use for a database source usually this is a person/s responsible for executing the analytical code.\n\nIndividual/patient level data\n\nPatient level data. i.e for each patient all the information. We only have access to patient level data for our datasets based within the team. We never have access to patient level data from other data partners.\n\nAggregated level data\n\nAs the name suggests the data only contains aggregated results such as results from a study. Aggregated data from data partners can be shared subject to the protocol and data sharing agreement with data partner.\n\nFederated network studies\n\nWhen the same analytical code is run across multiple databases/data partners across the UK, Europe, Globe etc in one study. Note we only share analytical code with a data partner and they run the code we never have access to individual/patient level data.\n\nEHDEN (European Health Data Evidence Network)\n\nThis is a large consortium with 25 partners operating in Europe. Note We are apart of this consortium and have led and participated in various federated network studies. More information here here.\n\nDARWIN\n\nThe European Medicines Agency (EMA) and the European Medicines Regulatory Network established a coordination centre to provide timely and reliable evidence on the use, safety and effectiveness of medicines for human use, including vaccines, from real world healthcare databases across the European Union (EU). This capability is called the Data Analysis and Real World Interrogation Network (DARWIN EU®). Dani is a deputy director. Note We lead and participate in studies that are requested by EMA. Furthermore, we develop R code and packages that are used in this project. More information here.\n\nInstantiating\n\nWhen you hear this term it basically means extracting and creating a table containing individuals with specified characteristics for example with a diagnosis of hypertension as a table in the database.",
    "crumbs": [
      "Onboarding",
      "Useful terms and terminologies"
    ]
  },
  {
    "objectID": "onboarding/index.html",
    "href": "onboarding/index.html",
    "title": "Onboarding",
    "section": "",
    "text": "This section is for people joining the team…",
    "crumbs": [
      "Onboarding"
    ]
  },
  {
    "objectID": "onboarding/work_with_me.html",
    "href": "onboarding/work_with_me.html",
    "title": "Work with me",
    "section": "",
    "text": "Work with me\nIn this section each one of the members of the group described themselves, their role in the team, and some details how to work with them.\n\nEd Burn\n\n\n\n\nHi I am Ed …\n\nFacts: loves being at home\nThe best way to contact me: When in office or github issue\nWhat I value: peace\n\n\n\n\n\nMartí Català\n\n\n\n\nHi I am Martí…\n\nFacts: hates small talk and loves catalunya.\nThe best way to contact me: If I am in the office just come to speak with me. Otherwise try Teams. If I don’t answer in the first 5 hours good luck with that… try it again :)\nWhat I value: honesty and anything catalan.\n\n\n\n\n\nDanielle Newby\n\n\n\n\nHi I am Danielle…\n\nFacts: loves small talk, sleeping and eating. She doesn’t like hugs.\nThe best way to contact me: Teams\nWhat I value: honesty and people asking questions. People not hugging me.\n\n\n\n\n\nMike Du\n\n\n\n\nHi I am Mike…\n\n\n\n\nYuchen Guo\n\n\n\n\nHi I am Mimi…\n\n\n\n\nTheresa Burkard\n\n\n\n\nHi I am Theresa\n\n\n\n\nXihang Chen\n\n\n\n\nHi I am Xihang…\n\n\n\n\nKim Lopez-Guell\n\n\n\n\nHi I am Kim…\n\n\n\n\nPablo Spivakovsky Gonzalez\nHi I am Pablo…\n\n\nMarta Alcalde-Herraiz\n\n\n\n\nHi I am Marta…\n\n\n\n\nNuria Mercade-Besora\nHi I am Nuria…\n\n\nElin Rowlands\nHi I am Elin…\n\n\nCecilia Campanile\nHi I am Cecilia…",
    "crumbs": [
      "Onboarding",
      "Work with me"
    ]
  },
  {
    "objectID": "onboarding/checklist.html",
    "href": "onboarding/checklist.html",
    "title": "Checklist",
    "section": "",
    "text": "Checklist\nHave you completed the following tasks?\n\nCreated a GitHub account\nBeen added to the PDE GitHub account\nBeen added to PDE teams channel and relevant channels (OHDSI server users, Oxinfer, DARWIN etc)\nGet access to intranet to get documents for information governance training\nCompleted your information governance online training\nContacted information governance manager to attend group information governance induction meeting\nRegister and complete CPRD database training (CPRD E-Learning module)\nRegister yourself as a NDORMS CPRD user\nGot your user account for the servers\nSet up your R studio environment\nCompleted mandatory health and safety training\nWrite a brief description of yourself and how you like to work to be included in the group website.\nDownload GitHub Desktop.\nBeen invited to the PDE meetings (Mahki).\nBeen invited to the OxInfer meetings (Ed).",
    "crumbs": [
      "Onboarding",
      "Checklist"
    ]
  },
  {
    "objectID": "onboarding/connect_to_database.html",
    "href": "onboarding/connect_to_database.html",
    "title": "Connect to the database",
    "section": "",
    "text": "To connect to databases we will use DBI package and CDMConnector, you can find more information about both packages in their websites:\n\nDBI package website: https://dbi.r-dbi.org/\nCDMConnector package website: https://darwin-eu.github.io/CDMConnector/\n\nConnect to database (standard way) and set up the environment\n\n\n\nThe following libraries will be used in this chapter: DBI, RPostgres, dplyr, dbplyr, usethis and here. If you do not have them installed you can install them with the following command:\n\ninstall.packages(c(\"DBI\", \"RPostgres\", \"dplyr\", \"dbplyr\", \"usethis\", \"here\"))\n\n\nlibrary(DBI)\nlibrary(RPostgres)\nlibrary(dplyr)\nlibrary(dbplyr)\nlibrary(usethis)\nlibrary(here)\n\n\n\n\nTo create a connection to a database we need some parameters:\n\nhost: it is the IP of the computer that contains the database in our case it will be always the same and you can check it here\nport: port to connect, in our case it will be always the same and you can check it here\ndbname: name of the database we want to connect, each database has a different name, link to the names of the databases hosted by our server: link\nuser: each individual has a user to connect to the database this is unique and nontransferable.\npassword: associated to each user.\n\nTo get a user and password or if you are not sure of what are the parameters of a certain database you can ask Hez.\n\n\n\nThere are several ways to create a connection as seen here and this depends on the Database Management System (DBMS) of your back-end.\nIn our case for the moment all our databases are in PostgreSQL (also refereed as Postgres) one of the most popular free dbms that exist. To connect to a Postgres we have to populate with the following information the connection details:\n\ncon &lt;- dbConnect(drv = Postgres(),\n                 dbname = \"...\",\n                 host = \"...\",\n                 port = \"...\",\n                 user = \"...\",\n                 password = \"...\")\n\nTo test that the connection that you created works you can test the following command:\n\ntbl(con, sql(\"SELECT * FROM public.person limit 1\"))\n\n\n\nCreating CDM database\n/home/runner/work/OxinferOnboarding/OxinferOnboarding/GiBleed_5.3.zip\n\n\n# Source:   SQL [1 x 18]\n# Database: DuckDB v1.0.0 [unknown@Linux 6.5.0-1025-azure:R 4.4.1//tmp/RtmpepWWtl/file13d32f53704e.duckdb]\n  person_id gender_concept_id year_of_birth month_of_birth day_of_birth\n      &lt;int&gt;             &lt;int&gt;         &lt;int&gt;          &lt;int&gt;        &lt;int&gt;\n1         6              8532          1963             12           31\n# ℹ 13 more variables: birth_datetime &lt;dttm&gt;, race_concept_id &lt;int&gt;,\n#   ethnicity_concept_id &lt;int&gt;, location_id &lt;int&gt;, provider_id &lt;int&gt;,\n#   care_site_id &lt;int&gt;, person_source_value &lt;chr&gt;, gender_source_value &lt;chr&gt;,\n#   gender_source_concept_id &lt;int&gt;, race_source_value &lt;chr&gt;,\n#   race_source_concept_id &lt;int&gt;, ethnicity_source_value &lt;chr&gt;,\n#   ethnicity_source_concept_id &lt;int&gt;\n\n\nNote that the result will be different as this one is from a synthetic database.\nWe can have multiple connections open at the same time, but it is recommended to not open more than one connection simultaneously, and close connections when we finish using them.\nOnce checked that we are able to connect to the database we can disconnect:\n\ndbDisconnect(conn = con)\n\n\n\n\nDatabases are organised in schemas. Schemas are the internal structure of a database, they are like “folders”. In general when we work with an OMOP database we will have to schemas:\n\ncdmSchema this schema contains all the OMOP Standard tables. Typically we would only have reading permissions to this schema, because these tables are not meant to be edited.\nwriteSchema this schema is usually empty and is for the user to save their tables (usually cohorts) of intermediate results that you want to keep or reuse later. Typically you would have writing and reading permissions for this schema. In our database we can only read and edit the tables that we created, so we can not edit or read other people tables. It is a good practice to write your own tables with a starting prefix so it is easier to avoid name conflicts.\n\nFor our databases the cdmSchema =public and the writeSchema =results.\n\n\n\nThe cdm object is a structure on top of our database connection to access all the tables in a user-friendly way, see more information in the CDMConnector website\nTo create our first cdm object we need first a connection to our back-end:\n\ncon &lt;- dbConnect(drv = Postgres(),\n                 dbname = \"...\",\n                 host = \"...\",\n                 port = \"...\",\n                 user = \"...\",\n                 password = \"...\")\n\nAnd then we can create the cdm object:\n\nlibrary(CDMConnector)\ncdm &lt;- cdmFromCon(con = con, cdmSchema = \"public\", writeSchema = \"results\")\n\nThe cdm object has a print that shows all the tables that you are connected to:\n\ncdm\n\n\n\n\n── # OMOP CDM reference (duckdb) of Synthea synthetic health database ──────────\n\n\n• omop tables: person, observation_period, visit_occurrence, visit_detail,\ncondition_occurrence, drug_exposure, procedure_occurrence, device_exposure,\nmeasurement, observation, death, note, note_nlp, specimen, fact_relationship,\nlocation, care_site, provider, payer_plan_period, cost, drug_era, dose_era,\ncondition_era, metadata, cdm_source, concept, vocabulary, domain,\nconcept_class, concept_relationship, relationship, concept_synonym,\nconcept_ancestor, source_to_concept_map, drug_strength\n\n\n• cohort tables: -\n\n\n• achilles tables: -\n\n\n• other tables: -\n\n\nAnd you can easily access to one of this tables with:\n\ncdm$person\n\n# Source:   table&lt;main.person&gt; [?? x 18]\n# Database: DuckDB v1.0.0 [unknown@Linux 6.5.0-1025-azure:R 4.4.1//tmp/RtmpepWWtl/file13d3536e85bd.duckdb]\n   person_id gender_concept_id year_of_birth month_of_birth day_of_birth\n       &lt;int&gt;             &lt;int&gt;         &lt;int&gt;          &lt;int&gt;        &lt;int&gt;\n 1         6              8532          1963             12           31\n 2       123              8507          1950              4           12\n 3       129              8507          1974             10            7\n 4        16              8532          1971             10           13\n 5        65              8532          1967              3           31\n 6        74              8532          1972              1            5\n 7        42              8532          1909             11            2\n 8       187              8507          1945              7           23\n 9        18              8532          1965             11           17\n10       111              8532          1975              5            2\n# ℹ more rows\n# ℹ 13 more variables: birth_datetime &lt;dttm&gt;, race_concept_id &lt;int&gt;,\n#   ethnicity_concept_id &lt;int&gt;, location_id &lt;int&gt;, provider_id &lt;int&gt;,\n#   care_site_id &lt;int&gt;, person_source_value &lt;chr&gt;, gender_source_value &lt;chr&gt;,\n#   gender_source_concept_id &lt;int&gt;, race_source_value &lt;chr&gt;,\n#   race_source_concept_id &lt;int&gt;, ethnicity_source_value &lt;chr&gt;,\n#   ethnicity_source_concept_id &lt;int&gt;\n\n\nIn the other tutorials you will learn more how to program and use the cdm object.\nOnce you finished using it you can close the connection of the cdm object with:\n\ncdmDisconnect(cdm)\n\nThis is equivalent to do:\n\ndbDisconnect(con)\n\n\n\n\nThe database administrator will give us a password it is extremely important that you change it after you connect to the database for first time. To change your password you must:\n\nopen a connection to any of our databases:\n\n\ncon &lt;- dbConnect(drv = Postgres(),\n                 dbname = \"...\", # here you can connect to any of our databases\n                 host = \"...\",\n                 port = \"...\",\n                 user = \"...\",\n                 password = \"...\")\n\n\nChange the password, to do so we have to run the following command:\n\n\ndbGetQuery(con, \"ALTER USER xxxxx WITH PASSWORD 'xxxxxxxxxxx'\")\n\nExample:\n\ndbGetQuery(con, \"ALTER USER martics WITH PASSWORD '12345678'\")\n\nMake sure that you chose an strong password, see the University guide for more information about strong and safe passwords.\n\nDisconnect from the database:\n\n\ndbDisconnect(con)\n\n\nConnect again with the new password to check that the change was done correctly:\n\n\ncon &lt;- dbConnect(drv = Postgres(),\n                 dbname = \"...\",\n                 host = \"...\",\n                 port = \"...\",\n                 user = \"...\",\n                 password = \"new_password\")\n\n\nDisconnect from the database again:\n\n\ndbDisconnect(con)\n\n*NOTE: even you connect to a particular database to change the password, username and password are unique for all databases, so changing it only once changes for any dbname in our evironment.\n\n\n\n\n\nConnection to the database it is a repetitive process that you will repeat every day, but at the same time you need to keep all your database credentials safely guarded. The R environ file will help use to do so.\n.Renviron is a file linked to your R session to safely save secrets. It is loaded at the beginning of each session and secrets are kept till the session is terminated or restarted.\nTo access to a secret you can type:\n\nSys.getenv(\"MY_SECRET\")\n\n[1] \"\"\n\n\nAs we have just seen if a secret does not exist the output will be an empty string: \"\".\nYou can set a temporary secret using the following command:\n\nSys.setenv(MY_SECRET = \"123456789\")\n\nThen now if you run the same command than before you will get the value that we have just set back:\n\nSys.getenv(\"MY_SECRET\")\n\n[1] \"123456789\"\n\n\nThis value can be assigned to a variable as we would do as usual:\n\nx &lt;- \"123456789\"\nprint(x)\n\n[1] \"123456789\"\n\n\n\ny &lt;- Sys.getenv(\"MY_SECRET\")\nprint(y)\n\n[1] \"123456789\"\n\n\nAs you can see x and y have the same value but if you share the code in the second case the code would only work if you have the same secret stored in your environment. This is very useful to store the connection details so even if you share your code you would not be share sensitive information.\nThere are two type of secrets:\n\ntemporal (as we have just seen), these secrets only last for while the session is ongoing the moment that you restart or terminate it all the secrets will be gone, so it is not recommended to use temporal secrets.\npermanent (we will see in the next step), these secrets are kept across different sessions and will always be in your environment unless you explicitly delete them. These secrets are stored in the .Renviron file. We will use permanent secrets to store our database credentials.\n\nYou can read more about secrets here.\n*NOTE: you can see all the secrets of your R session running the following command:\n\nSys.getenv()\n\n\n\n\nTo open your .Renviron file you can run the following command:\n\nedit_r_environ()\n\nThere you can write secrets that you want to store\n\nMY_SECRET = \"123456789\"\n\nIt is like writing in the Sys.setenv() function, but these secrets will be permanent and always be loaded in your session.\nSecrets are loaded every time our session starts, so if we modify the .Renviron file and we want the new secrets to be loaded we should restart the R session (Control+Shift+F10).\nTry writing your username, password, port and host there and create a connection with the following code:\n\ncon &lt;- dbConnect(drv = Postgres(),\n                 dbname = \"...\",\n                 host = Sys.getenv(\"HOST\"),\n                 port = Sys.getenv(\"PORT\"),\n                 user = Sys.getenv(\"USER\"),\n                 password = Sys.getenv(\"PASSWORD\"))\n\n\n\n\n\nYOU MUST NEVER INCLUDE CONNECTION DETAILS IN A SCRIPT THAT IS PUBLIC OR YOU SHARE WITH SOMEONE, EVEN IF YOU TRUST THAT PERSON OR IS IN THE SAME ORGANIZATION.",
    "crumbs": [
      "Onboarding",
      "Connect to the database"
    ]
  },
  {
    "objectID": "onboarding/connect_to_database.html#getting-started",
    "href": "onboarding/connect_to_database.html#getting-started",
    "title": "Connect to the database",
    "section": "",
    "text": "To connect to databases we will use DBI package and CDMConnector, you can find more information about both packages in their websites:\n\nDBI package website: https://dbi.r-dbi.org/\nCDMConnector package website: https://darwin-eu.github.io/CDMConnector/\n\nConnect to database (standard way) and set up the environment",
    "crumbs": [
      "Onboarding",
      "Connect to the database"
    ]
  },
  {
    "objectID": "onboarding/connect_to_database.html#load-libraries",
    "href": "onboarding/connect_to_database.html#load-libraries",
    "title": "Connect to the database",
    "section": "",
    "text": "The following libraries will be used in this chapter: DBI, RPostgres, dplyr, dbplyr, usethis and here. If you do not have them installed you can install them with the following command:\n\ninstall.packages(c(\"DBI\", \"RPostgres\", \"dplyr\", \"dbplyr\", \"usethis\", \"here\"))\n\n\nlibrary(DBI)\nlibrary(RPostgres)\nlibrary(dplyr)\nlibrary(dbplyr)\nlibrary(usethis)\nlibrary(here)",
    "crumbs": [
      "Onboarding",
      "Connect to the database"
    ]
  },
  {
    "objectID": "onboarding/connect_to_database.html#credentials-and-database-details",
    "href": "onboarding/connect_to_database.html#credentials-and-database-details",
    "title": "Connect to the database",
    "section": "",
    "text": "To create a connection to a database we need some parameters:\n\nhost: it is the IP of the computer that contains the database in our case it will be always the same and you can check it here\nport: port to connect, in our case it will be always the same and you can check it here\ndbname: name of the database we want to connect, each database has a different name, link to the names of the databases hosted by our server: link\nuser: each individual has a user to connect to the database this is unique and nontransferable.\npassword: associated to each user.\n\nTo get a user and password or if you are not sure of what are the parameters of a certain database you can ask Hez.",
    "crumbs": [
      "Onboarding",
      "Connect to the database"
    ]
  },
  {
    "objectID": "onboarding/connect_to_database.html#create-a-connection",
    "href": "onboarding/connect_to_database.html#create-a-connection",
    "title": "Connect to the database",
    "section": "",
    "text": "There are several ways to create a connection as seen here and this depends on the Database Management System (DBMS) of your back-end.\nIn our case for the moment all our databases are in PostgreSQL (also refereed as Postgres) one of the most popular free dbms that exist. To connect to a Postgres we have to populate with the following information the connection details:\n\ncon &lt;- dbConnect(drv = Postgres(),\n                 dbname = \"...\",\n                 host = \"...\",\n                 port = \"...\",\n                 user = \"...\",\n                 password = \"...\")\n\nTo test that the connection that you created works you can test the following command:\n\ntbl(con, sql(\"SELECT * FROM public.person limit 1\"))\n\n\n\nCreating CDM database\n/home/runner/work/OxinferOnboarding/OxinferOnboarding/GiBleed_5.3.zip\n\n\n# Source:   SQL [1 x 18]\n# Database: DuckDB v1.0.0 [unknown@Linux 6.5.0-1025-azure:R 4.4.1//tmp/RtmpepWWtl/file13d32f53704e.duckdb]\n  person_id gender_concept_id year_of_birth month_of_birth day_of_birth\n      &lt;int&gt;             &lt;int&gt;         &lt;int&gt;          &lt;int&gt;        &lt;int&gt;\n1         6              8532          1963             12           31\n# ℹ 13 more variables: birth_datetime &lt;dttm&gt;, race_concept_id &lt;int&gt;,\n#   ethnicity_concept_id &lt;int&gt;, location_id &lt;int&gt;, provider_id &lt;int&gt;,\n#   care_site_id &lt;int&gt;, person_source_value &lt;chr&gt;, gender_source_value &lt;chr&gt;,\n#   gender_source_concept_id &lt;int&gt;, race_source_value &lt;chr&gt;,\n#   race_source_concept_id &lt;int&gt;, ethnicity_source_value &lt;chr&gt;,\n#   ethnicity_source_concept_id &lt;int&gt;\n\n\nNote that the result will be different as this one is from a synthetic database.\nWe can have multiple connections open at the same time, but it is recommended to not open more than one connection simultaneously, and close connections when we finish using them.\nOnce checked that we are able to connect to the database we can disconnect:\n\ndbDisconnect(conn = con)",
    "crumbs": [
      "Onboarding",
      "Connect to the database"
    ]
  },
  {
    "objectID": "onboarding/connect_to_database.html#database-schemas",
    "href": "onboarding/connect_to_database.html#database-schemas",
    "title": "Connect to the database",
    "section": "",
    "text": "Databases are organised in schemas. Schemas are the internal structure of a database, they are like “folders”. In general when we work with an OMOP database we will have to schemas:\n\ncdmSchema this schema contains all the OMOP Standard tables. Typically we would only have reading permissions to this schema, because these tables are not meant to be edited.\nwriteSchema this schema is usually empty and is for the user to save their tables (usually cohorts) of intermediate results that you want to keep or reuse later. Typically you would have writing and reading permissions for this schema. In our database we can only read and edit the tables that we created, so we can not edit or read other people tables. It is a good practice to write your own tables with a starting prefix so it is easier to avoid name conflicts.\n\nFor our databases the cdmSchema =public and the writeSchema =results.",
    "crumbs": [
      "Onboarding",
      "Connect to the database"
    ]
  },
  {
    "objectID": "onboarding/connect_to_database.html#create-the-cdm-object",
    "href": "onboarding/connect_to_database.html#create-the-cdm-object",
    "title": "Connect to the database",
    "section": "",
    "text": "The cdm object is a structure on top of our database connection to access all the tables in a user-friendly way, see more information in the CDMConnector website\nTo create our first cdm object we need first a connection to our back-end:\n\ncon &lt;- dbConnect(drv = Postgres(),\n                 dbname = \"...\",\n                 host = \"...\",\n                 port = \"...\",\n                 user = \"...\",\n                 password = \"...\")\n\nAnd then we can create the cdm object:\n\nlibrary(CDMConnector)\ncdm &lt;- cdmFromCon(con = con, cdmSchema = \"public\", writeSchema = \"results\")\n\nThe cdm object has a print that shows all the tables that you are connected to:\n\ncdm\n\n\n\n\n── # OMOP CDM reference (duckdb) of Synthea synthetic health database ──────────\n\n\n• omop tables: person, observation_period, visit_occurrence, visit_detail,\ncondition_occurrence, drug_exposure, procedure_occurrence, device_exposure,\nmeasurement, observation, death, note, note_nlp, specimen, fact_relationship,\nlocation, care_site, provider, payer_plan_period, cost, drug_era, dose_era,\ncondition_era, metadata, cdm_source, concept, vocabulary, domain,\nconcept_class, concept_relationship, relationship, concept_synonym,\nconcept_ancestor, source_to_concept_map, drug_strength\n\n\n• cohort tables: -\n\n\n• achilles tables: -\n\n\n• other tables: -\n\n\nAnd you can easily access to one of this tables with:\n\ncdm$person\n\n# Source:   table&lt;main.person&gt; [?? x 18]\n# Database: DuckDB v1.0.0 [unknown@Linux 6.5.0-1025-azure:R 4.4.1//tmp/RtmpepWWtl/file13d3536e85bd.duckdb]\n   person_id gender_concept_id year_of_birth month_of_birth day_of_birth\n       &lt;int&gt;             &lt;int&gt;         &lt;int&gt;          &lt;int&gt;        &lt;int&gt;\n 1         6              8532          1963             12           31\n 2       123              8507          1950              4           12\n 3       129              8507          1974             10            7\n 4        16              8532          1971             10           13\n 5        65              8532          1967              3           31\n 6        74              8532          1972              1            5\n 7        42              8532          1909             11            2\n 8       187              8507          1945              7           23\n 9        18              8532          1965             11           17\n10       111              8532          1975              5            2\n# ℹ more rows\n# ℹ 13 more variables: birth_datetime &lt;dttm&gt;, race_concept_id &lt;int&gt;,\n#   ethnicity_concept_id &lt;int&gt;, location_id &lt;int&gt;, provider_id &lt;int&gt;,\n#   care_site_id &lt;int&gt;, person_source_value &lt;chr&gt;, gender_source_value &lt;chr&gt;,\n#   gender_source_concept_id &lt;int&gt;, race_source_value &lt;chr&gt;,\n#   race_source_concept_id &lt;int&gt;, ethnicity_source_value &lt;chr&gt;,\n#   ethnicity_source_concept_id &lt;int&gt;\n\n\nIn the other tutorials you will learn more how to program and use the cdm object.\nOnce you finished using it you can close the connection of the cdm object with:\n\ncdmDisconnect(cdm)\n\nThis is equivalent to do:\n\ndbDisconnect(con)",
    "crumbs": [
      "Onboarding",
      "Connect to the database"
    ]
  },
  {
    "objectID": "onboarding/connect_to_database.html#change-password",
    "href": "onboarding/connect_to_database.html#change-password",
    "title": "Connect to the database",
    "section": "",
    "text": "The database administrator will give us a password it is extremely important that you change it after you connect to the database for first time. To change your password you must:\n\nopen a connection to any of our databases:\n\n\ncon &lt;- dbConnect(drv = Postgres(),\n                 dbname = \"...\", # here you can connect to any of our databases\n                 host = \"...\",\n                 port = \"...\",\n                 user = \"...\",\n                 password = \"...\")\n\n\nChange the password, to do so we have to run the following command:\n\n\ndbGetQuery(con, \"ALTER USER xxxxx WITH PASSWORD 'xxxxxxxxxxx'\")\n\nExample:\n\ndbGetQuery(con, \"ALTER USER martics WITH PASSWORD '12345678'\")\n\nMake sure that you chose an strong password, see the University guide for more information about strong and safe passwords.\n\nDisconnect from the database:\n\n\ndbDisconnect(con)\n\n\nConnect again with the new password to check that the change was done correctly:\n\n\ncon &lt;- dbConnect(drv = Postgres(),\n                 dbname = \"...\",\n                 host = \"...\",\n                 port = \"...\",\n                 user = \"...\",\n                 password = \"new_password\")\n\n\nDisconnect from the database again:\n\n\ndbDisconnect(con)\n\n*NOTE: even you connect to a particular database to change the password, username and password are unique for all databases, so changing it only once changes for any dbname in our evironment.",
    "crumbs": [
      "Onboarding",
      "Connect to the database"
    ]
  },
  {
    "objectID": "onboarding/connect_to_database.html#set-your-.renviron",
    "href": "onboarding/connect_to_database.html#set-your-.renviron",
    "title": "Connect to the database",
    "section": "",
    "text": "Connection to the database it is a repetitive process that you will repeat every day, but at the same time you need to keep all your database credentials safely guarded. The R environ file will help use to do so.\n.Renviron is a file linked to your R session to safely save secrets. It is loaded at the beginning of each session and secrets are kept till the session is terminated or restarted.\nTo access to a secret you can type:\n\nSys.getenv(\"MY_SECRET\")\n\n[1] \"\"\n\n\nAs we have just seen if a secret does not exist the output will be an empty string: \"\".\nYou can set a temporary secret using the following command:\n\nSys.setenv(MY_SECRET = \"123456789\")\n\nThen now if you run the same command than before you will get the value that we have just set back:\n\nSys.getenv(\"MY_SECRET\")\n\n[1] \"123456789\"\n\n\nThis value can be assigned to a variable as we would do as usual:\n\nx &lt;- \"123456789\"\nprint(x)\n\n[1] \"123456789\"\n\n\n\ny &lt;- Sys.getenv(\"MY_SECRET\")\nprint(y)\n\n[1] \"123456789\"\n\n\nAs you can see x and y have the same value but if you share the code in the second case the code would only work if you have the same secret stored in your environment. This is very useful to store the connection details so even if you share your code you would not be share sensitive information.\nThere are two type of secrets:\n\ntemporal (as we have just seen), these secrets only last for while the session is ongoing the moment that you restart or terminate it all the secrets will be gone, so it is not recommended to use temporal secrets.\npermanent (we will see in the next step), these secrets are kept across different sessions and will always be in your environment unless you explicitly delete them. These secrets are stored in the .Renviron file. We will use permanent secrets to store our database credentials.\n\nYou can read more about secrets here.\n*NOTE: you can see all the secrets of your R session running the following command:\n\nSys.getenv()\n\n\n\n\nTo open your .Renviron file you can run the following command:\n\nedit_r_environ()\n\nThere you can write secrets that you want to store\n\nMY_SECRET = \"123456789\"\n\nIt is like writing in the Sys.setenv() function, but these secrets will be permanent and always be loaded in your session.\nSecrets are loaded every time our session starts, so if we modify the .Renviron file and we want the new secrets to be loaded we should restart the R session (Control+Shift+F10).\nTry writing your username, password, port and host there and create a connection with the following code:\n\ncon &lt;- dbConnect(drv = Postgres(),\n                 dbname = \"...\",\n                 host = Sys.getenv(\"HOST\"),\n                 port = Sys.getenv(\"PORT\"),\n                 user = Sys.getenv(\"USER\"),\n                 password = Sys.getenv(\"PASSWORD\"))",
    "crumbs": [
      "Onboarding",
      "Connect to the database"
    ]
  },
  {
    "objectID": "onboarding/connect_to_database.html#final-remark",
    "href": "onboarding/connect_to_database.html#final-remark",
    "title": "Connect to the database",
    "section": "",
    "text": "YOU MUST NEVER INCLUDE CONNECTION DETAILS IN A SCRIPT THAT IS PUBLIC OR YOU SHARE WITH SOMEONE, EVEN IF YOU TRUST THAT PERSON OR IS IN THE SAME ORGANIZATION.",
    "crumbs": [
      "Onboarding",
      "Connect to the database"
    ]
  },
  {
    "objectID": "onboarding/useful_contacts.html",
    "href": "onboarding/useful_contacts.html",
    "title": "Useful people and things to know",
    "section": "",
    "text": "Useful people and things to know\nBelow are useful people to know in the team when you get stuck:\n\nHave a problem with the servers? Hez.\nNeed access to a database?\n\nInformation governance training: Lydia Underdown\nSpecific access to database: Hez. & Antonella (you must get approval from your line manager and cc your line manager when requesting access to databases)\n\nNeed access to our GitHub? Marti, Ed, Danielle\nNeed access to teams channels? Mahki or line manager\nHave a question about a database and/or mapping? Teen & Antonella\nNeed help booking flights/accommodation/admin/submitting a paper? Mahki, Sachi, Francesca (on mat leave)\nWork related issue? Line manager and/or HR\nHealth related issue? Line manager and/or HR\n\nHolidays: Please take all your entitled annual leave otherwise you will lose it!. At the moment, for wellbeing of individuals, the department will NOT allow you to carry over leave to the next year apart from exceptional circumstances (holiday year runs from 1st October to 30th Sept).\nOverworking: As a team we do not endorse overworking i.e. working outside normal working hours, weekends etc. If you receive emails/teams notifications outside of normal working it is NOT expected for you to respond (unless it is life or death..which work rarely is).",
    "crumbs": [
      "Onboarding",
      "Useful people and things to know"
    ]
  },
  {
    "objectID": "onboarding/organigram.html",
    "href": "onboarding/organigram.html",
    "title": "Organigram",
    "section": "",
    "text": "Organigram\nAlthough you might be in a certain team within the group we constantly work with people in other teams. This is why we provide training on both the epidemiology and data science to give everyone a well-rounded understanding to produce world leading research 😊. As you can see in the organagram below Dani leads the group with Ed, Annika, Mahki, Trishna and Antonella leading specific teams/themes in the team.\n\n\n\nTeam Structure Jul 24",
    "crumbs": [
      "Onboarding",
      "Organigram"
    ]
  },
  {
    "objectID": "onboarding/databases_and_servers.html",
    "href": "onboarding/databases_and_servers.html",
    "title": "Databases and the servers",
    "section": "",
    "text": "We have access to a few different databases in the team (as of ?meta:date). However, access will be provided based on the projects you are working on and completion of training. Databases we have access too:\n\nCPRD GOLD and AURUM (primary care data from the UK)\nTHIN (UK, France, Belgium, Spain, Romania, Italy)\nUKBiobank (UK)\n\nTo get access to any of these databases you will need to complete the information governance training (links on the intranet but also here). This involves completing the online training and face to face training. Lydia Underdown is the Governance manager in the department.\nFor CPRD database access you will need to complete additional training provided by them and register at ndorms as a user of CPRD. See here on the intranet for more information or you can access directly here. You will also need to create an account on CPRD eRap and then be added onto a new or existing application to be granted access to CPRD on our servers. Things to note:\n\nAntonella is the key fob holder to CPRD for the department.\nAntonella needs to be including as a collaborator on ALL CPRD applications you make to CPRD.\nAntonella and whoever mapped the database need to be added as co-authors for publications.\n\nFor THIN, you will need to be added onto an approved protocol and/or write one for submission. Your line manager will be able to help with this. There is also a dedicated teams channel for THIN applications. The main people for THIN are Danielle and Antonella.\nFor UKBiobank you will need to register and complete the application (you will need to provide a CV). The main users for UKBiobank are Frank, Marta Jnr and Danielle. NOTE for new applications of UKB data you will have to do the analysis on their dedicated servers.\nOnce you have completed the training you can then contact Hez and Antonella about getting access on our servers to the specific database cc’ing your line manager in. When requesting access please specify which data cut you require and if you want both Rstudio and ATLAS access for the database in question. If you are unsure speak to your line manager.\n\n\n\nWe have two servers with a R studio interface one for running your final code and one for running code that is under development. When we develop code for studies we only use a subset of a database and we do not run on the main one until we have tested our code. This subset is a random 100k people and is provided for each database we have. Please use the 100k when developing code on the development server. Links to the servers can be found below:\n\nStudy/main server\nDevelopment server\n\nNOTE if you are on a laptop you need to be connected to University’s virtual private network (VPN)\nDocuments for helping to configure Rstudio and GitHub on the servers can be found in other chapters in this book or here.\n\n\n\nSometimes we need to see what jobs are running on the servers. If you have a desktop you can access a tool which shows what jobs are running on the servers and by who. NOTE: this tool does not work on laptops due to firewall restrictions by the university. The link is here.\nIf you do not have access to this, you can check what jobs you have running on the server via RStudio and how to kill these jobs. You can also ask Hez to kill jobs.",
    "crumbs": [
      "Onboarding",
      "Databases and the servers"
    ]
  },
  {
    "objectID": "onboarding/databases_and_servers.html#databases",
    "href": "onboarding/databases_and_servers.html#databases",
    "title": "Databases and the servers",
    "section": "",
    "text": "We have access to a few different databases in the team (as of ?meta:date). However, access will be provided based on the projects you are working on and completion of training. Databases we have access too:\n\nCPRD GOLD and AURUM (primary care data from the UK)\nTHIN (UK, France, Belgium, Spain, Romania, Italy)\nUKBiobank (UK)\n\nTo get access to any of these databases you will need to complete the information governance training (links on the intranet but also here). This involves completing the online training and face to face training. Lydia Underdown is the Governance manager in the department.\nFor CPRD database access you will need to complete additional training provided by them and register at ndorms as a user of CPRD. See here on the intranet for more information or you can access directly here. You will also need to create an account on CPRD eRap and then be added onto a new or existing application to be granted access to CPRD on our servers. Things to note:\n\nAntonella is the key fob holder to CPRD for the department.\nAntonella needs to be including as a collaborator on ALL CPRD applications you make to CPRD.\nAntonella and whoever mapped the database need to be added as co-authors for publications.\n\nFor THIN, you will need to be added onto an approved protocol and/or write one for submission. Your line manager will be able to help with this. There is also a dedicated teams channel for THIN applications. The main people for THIN are Danielle and Antonella.\nFor UKBiobank you will need to register and complete the application (you will need to provide a CV). The main users for UKBiobank are Frank, Marta Jnr and Danielle. NOTE for new applications of UKB data you will have to do the analysis on their dedicated servers.\nOnce you have completed the training you can then contact Hez and Antonella about getting access on our servers to the specific database cc’ing your line manager in. When requesting access please specify which data cut you require and if you want both Rstudio and ATLAS access for the database in question. If you are unsure speak to your line manager.",
    "crumbs": [
      "Onboarding",
      "Databases and the servers"
    ]
  },
  {
    "objectID": "onboarding/databases_and_servers.html#the-servers",
    "href": "onboarding/databases_and_servers.html#the-servers",
    "title": "Databases and the servers",
    "section": "",
    "text": "We have two servers with a R studio interface one for running your final code and one for running code that is under development. When we develop code for studies we only use a subset of a database and we do not run on the main one until we have tested our code. This subset is a random 100k people and is provided for each database we have. Please use the 100k when developing code on the development server. Links to the servers can be found below:\n\nStudy/main server\nDevelopment server\n\nNOTE if you are on a laptop you need to be connected to University’s virtual private network (VPN)\nDocuments for helping to configure Rstudio and GitHub on the servers can be found in other chapters in this book or here.",
    "crumbs": [
      "Onboarding",
      "Databases and the servers"
    ]
  },
  {
    "objectID": "onboarding/databases_and_servers.html#jobs-on-the-servers",
    "href": "onboarding/databases_and_servers.html#jobs-on-the-servers",
    "title": "Databases and the servers",
    "section": "",
    "text": "Sometimes we need to see what jobs are running on the servers. If you have a desktop you can access a tool which shows what jobs are running on the servers and by who. NOTE: this tool does not work on laptops due to firewall restrictions by the university. The link is here.\nIf you do not have access to this, you can check what jobs you have running on the server via RStudio and how to kill these jobs. You can also ask Hez to kill jobs.",
    "crumbs": [
      "Onboarding",
      "Databases and the servers"
    ]
  },
  {
    "objectID": "onboarding/code_review.html",
    "href": "onboarding/code_review.html",
    "title": "Code Review",
    "section": "",
    "text": "Code Review\nSome useful text here to explain who and what and when this should be done.. TBC\n\nBasic checks:\n\n\nCode is on github\nStudy is organised as an R project\nRenv is used to list all dependencies needed\nStudy code has a clear logical flow, with any particularly long scripts split up into separate files\nStudy code doesn’t have a lot of complex, custom code (that should be in a package with tests)\nThe code runs on a 100k dataset without error\nHow are the results visualised and reported?\nIs there a shiny to go with the study code?\nReview results for plausibility\n\n\nCheck whether the code does what is intended:\n\n\nDoes the code match the protocol?\nHave any analyses been missed?\nFor each analysis, are cohorts defined in the right way (e.g. typically no exclusion criteria for an incidence outcome) - this has been the most common source of issues\n\n\nCheck whether the code can be optimised:\n\n\nIs any code repeated unnecessarily?\nCan code be simplified?\nReview the sql that gets executed for any obvious inefficiencies",
    "crumbs": [
      "Onboarding",
      "Code Review"
    ]
  },
  {
    "objectID": "onboarding/omop.html",
    "href": "onboarding/omop.html",
    "title": "OMOP the basis",
    "section": "",
    "text": "New to the OMOP CDM? We’d recommend you pare this book with The Book of OHDSI\n\n\nAll our studies are done with what it is called Real World Data (RWD). It refers to data collected from various sources outside of traditional clinical trials. It encompasses information about the health status, treatment, and outcomes of patients in real-world settings.\nFor example, the most common source that we use is Electronic Health Records (EHRs): Data collected from healthcare providers during routine clinical care, including patient demographics, diagnoses, treatments, and outcomes.\nThere exist also other sources of RWD: Claims and Billing Activities (from health insurance claims), Registries (databases that collect information on patients with specific diseases or conditions), Pharmacy Data (prescription medications), among others…\n\n\nRWD can be used in many different ways:\n\nRegulatory Decision Making: Regulatory agencies, like the FDA, use RWD to support approval of new treatments, label expansions, and post-market surveillance.\nClinical Decision Support: RWD helps healthcare providers make more informed decisions about patient care by providing insights into treatment effectiveness and safety in broader populations.\nHealth Economics and Outcomes Research (HEOR): RWD is used to assess the cost-effectiveness and value of medical interventions.\nEpidemiology: RWD aids in understanding the prevalence, incidence, and burden of diseases in the population.\nComparative Effectiveness Research: Researchers use RWD to compare the effectiveness of different treatments in real-world settings.\nPharmacovigilance: RWD is crucial for monitoring the safety of medications post-approval to identify and mitigate adverse effects.\n\n\n\n\nWhen working with RWD we have to be aware of some challenges and limitations:\n\nData Quality: Inconsistencies, missing information, and variations in data collection methods can affect the reliability of RWD.\nData Integration: Combining data from diverse sources can be complex due to differences in formats, standards, and terminologies.\nPrivacy and Security: Ensuring patient confidentiality and data security is paramount when handling RWD.\nBias and Confounding: Real-world studies may be subject to biases and confounders that can impact the validity of findings.\n\n\n\n\nWe call the evidence generated by RWD as Real World Evidence. This will be our end goal, to generate reliable Real World Evidence in a transparent and fast way.\n\n\n\n\nAs we have just seen RWD can come from many different sources and it is usually not collected for research purposes. This leads to diverse structures, coding systems and can become quite a nightmare to reproduce an study in different databases. This is why common data models started gaining popularity.\nUsing a common data model (CDM) is crucial for standardising and harmonising data from disparate sources, ensuring consistency and interoperability. A CDM facilitates the integration and analysis of data from various healthcare systems by providing a unified structure and standardised terminologies. This standardisation enables researchers and healthcare professionals to perform meaningful comparisons, aggregate data efficiently, and derive robust, generalisable insights. Additionally, a CDM enhances data quality and reliability, reduces the potential for errors, and supports regulatory compliance and collaborative research efforts. Ultimately, adopting a common data model accelerates the translation of real-world data into actionable knowledge, improving patient outcomes and advancing medical research.\n\n\n\nThe Observational Medical Outcomes Partnership (OMOP) is a common data model for organising healthcare data from various sources. It is ones of the most popular growing CDM world wide with more than 800 million patients’ health care data transformed into this format.\nThe OMOP CDM is a person-centric relational data model. Patients’ data is spread across various tables related to different clinical domains with, for example, the condition occurrence table containing diagnoses while the drug exposure table contains drug prescriptions. These different clinical tables are all linked back to the person table which contains a unique identifier for each individual along with some key demographic data such as their date of birth. Meanwhile, records in the observation period table define the period of calendar time over which an individual is followed-up.\nIn this figure you can see the different tables that exist in the OMOP CDM and how are they related:\n\n\n\n\nThe OHDSI (Observational Health Data Sciences and Informatics) community is a global, multi-stakeholder, interdisciplinary collaborative that aims to improve health by empowering the community to collaboratively generate evidence that promotes better health decisions and better care.\n\nThe primary mission is to create a research community that produces high-quality, reproducible, and reliable evidence about health and healthcare.\nA key component of OHDSI’s infrastructure is the OMOP (Observational Medical Outcomes Partnership) Common Data Model.\nOHDSI emphasizes open science principles, making their tools, methods, and research findings freely available to the public though their github: https://github.com/ohdsi.\nAnnual symposiums and other events foster collaboration and the sharing of ideas within the community.\n\nYou can find more about ohdsi community on their website: https://ohdsi.org.\n\n\n\nThe Ehden Academy is an educational initiative that contains lots of resources on the basis of OMOP and how the OMOP CDM is structured, its vocabularies and so…\nIn particular we would recommend these courses:\n\n…\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThat’s a brief introduction to a very complicated topic, please refer to the provided links to learn more and get a more in depth view.\n\n\nEvery record in a RWD database gets coded into a numeric identifier (code), there exist many different Medical Classifications that are different vocabularies. Each vocabulary has its pros and its cons. Each database generally will come with a different vocabulary. OMOP CDM has some standard vicabularies that are the ones commonly used. In general this vocabulary will be different to the source one (originl of your data).\nWe call mapping to the process to convert a source data (original format of the data) that can be in many different formats to the OMOP CDM. In our team Antonella and Teen are the ones in charge of the mapping process.\nAthena contains the last version of the vocabularies and it is used to track its changes. You can use athena to search for OMOP concepts and see how they are related.\n\n\n\nThis was a very general introduction to the OMOP CDM as there are many resources out there that can help you to familiarise with OMOP. We recomend to take a look to The Book of OHDSI and the recommended courses from Ehden Academy, but the best way to learn about OMOP is to do an study with one of our OMOP instances, learning by doing :).",
    "crumbs": [
      "Onboarding",
      "OMOP the basis"
    ]
  },
  {
    "objectID": "onboarding/omop.html#real-word-data",
    "href": "onboarding/omop.html#real-word-data",
    "title": "OMOP the basis",
    "section": "",
    "text": "All our studies are done with what it is called Real World Data (RWD). It refers to data collected from various sources outside of traditional clinical trials. It encompasses information about the health status, treatment, and outcomes of patients in real-world settings.\nFor example, the most common source that we use is Electronic Health Records (EHRs): Data collected from healthcare providers during routine clinical care, including patient demographics, diagnoses, treatments, and outcomes.\nThere exist also other sources of RWD: Claims and Billing Activities (from health insurance claims), Registries (databases that collect information on patients with specific diseases or conditions), Pharmacy Data (prescription medications), among others…\n\n\nRWD can be used in many different ways:\n\nRegulatory Decision Making: Regulatory agencies, like the FDA, use RWD to support approval of new treatments, label expansions, and post-market surveillance.\nClinical Decision Support: RWD helps healthcare providers make more informed decisions about patient care by providing insights into treatment effectiveness and safety in broader populations.\nHealth Economics and Outcomes Research (HEOR): RWD is used to assess the cost-effectiveness and value of medical interventions.\nEpidemiology: RWD aids in understanding the prevalence, incidence, and burden of diseases in the population.\nComparative Effectiveness Research: Researchers use RWD to compare the effectiveness of different treatments in real-world settings.\nPharmacovigilance: RWD is crucial for monitoring the safety of medications post-approval to identify and mitigate adverse effects.\n\n\n\n\nWhen working with RWD we have to be aware of some challenges and limitations:\n\nData Quality: Inconsistencies, missing information, and variations in data collection methods can affect the reliability of RWD.\nData Integration: Combining data from diverse sources can be complex due to differences in formats, standards, and terminologies.\nPrivacy and Security: Ensuring patient confidentiality and data security is paramount when handling RWD.\nBias and Confounding: Real-world studies may be subject to biases and confounders that can impact the validity of findings.\n\n\n\n\nWe call the evidence generated by RWD as Real World Evidence. This will be our end goal, to generate reliable Real World Evidence in a transparent and fast way.",
    "crumbs": [
      "Onboarding",
      "OMOP the basis"
    ]
  },
  {
    "objectID": "onboarding/omop.html#why-a-common-data-model",
    "href": "onboarding/omop.html#why-a-common-data-model",
    "title": "OMOP the basis",
    "section": "",
    "text": "As we have just seen RWD can come from many different sources and it is usually not collected for research purposes. This leads to diverse structures, coding systems and can become quite a nightmare to reproduce an study in different databases. This is why common data models started gaining popularity.\nUsing a common data model (CDM) is crucial for standardising and harmonising data from disparate sources, ensuring consistency and interoperability. A CDM facilitates the integration and analysis of data from various healthcare systems by providing a unified structure and standardised terminologies. This standardisation enables researchers and healthcare professionals to perform meaningful comparisons, aggregate data efficiently, and derive robust, generalisable insights. Additionally, a CDM enhances data quality and reliability, reduces the potential for errors, and supports regulatory compliance and collaborative research efforts. Ultimately, adopting a common data model accelerates the translation of real-world data into actionable knowledge, improving patient outcomes and advancing medical research.",
    "crumbs": [
      "Onboarding",
      "OMOP the basis"
    ]
  },
  {
    "objectID": "onboarding/omop.html#omop-basis",
    "href": "onboarding/omop.html#omop-basis",
    "title": "OMOP the basis",
    "section": "",
    "text": "The Observational Medical Outcomes Partnership (OMOP) is a common data model for organising healthcare data from various sources. It is ones of the most popular growing CDM world wide with more than 800 million patients’ health care data transformed into this format.\nThe OMOP CDM is a person-centric relational data model. Patients’ data is spread across various tables related to different clinical domains with, for example, the condition occurrence table containing diagnoses while the drug exposure table contains drug prescriptions. These different clinical tables are all linked back to the person table which contains a unique identifier for each individual along with some key demographic data such as their date of birth. Meanwhile, records in the observation period table define the period of calendar time over which an individual is followed-up.\nIn this figure you can see the different tables that exist in the OMOP CDM and how are they related:",
    "crumbs": [
      "Onboarding",
      "OMOP the basis"
    ]
  },
  {
    "objectID": "onboarding/omop.html#ohdsi",
    "href": "onboarding/omop.html#ohdsi",
    "title": "OMOP the basis",
    "section": "",
    "text": "The OHDSI (Observational Health Data Sciences and Informatics) community is a global, multi-stakeholder, interdisciplinary collaborative that aims to improve health by empowering the community to collaboratively generate evidence that promotes better health decisions and better care.\n\nThe primary mission is to create a research community that produces high-quality, reproducible, and reliable evidence about health and healthcare.\nA key component of OHDSI’s infrastructure is the OMOP (Observational Medical Outcomes Partnership) Common Data Model.\nOHDSI emphasizes open science principles, making their tools, methods, and research findings freely available to the public though their github: https://github.com/ohdsi.\nAnnual symposiums and other events foster collaboration and the sharing of ideas within the community.\n\nYou can find more about ohdsi community on their website: https://ohdsi.org.",
    "crumbs": [
      "Onboarding",
      "OMOP the basis"
    ]
  },
  {
    "objectID": "onboarding/omop.html#edhden-academy",
    "href": "onboarding/omop.html#edhden-academy",
    "title": "OMOP the basis",
    "section": "",
    "text": "The Ehden Academy is an educational initiative that contains lots of resources on the basis of OMOP and how the OMOP CDM is structured, its vocabularies and so…\nIn particular we would recommend these courses:\n\n…",
    "crumbs": [
      "Onboarding",
      "OMOP the basis"
    ]
  },
  {
    "objectID": "onboarding/omop.html#vocabularies",
    "href": "onboarding/omop.html#vocabularies",
    "title": "OMOP the basis",
    "section": "",
    "text": "Warning\n\n\n\nThat’s a brief introduction to a very complicated topic, please refer to the provided links to learn more and get a more in depth view.\n\n\nEvery record in a RWD database gets coded into a numeric identifier (code), there exist many different Medical Classifications that are different vocabularies. Each vocabulary has its pros and its cons. Each database generally will come with a different vocabulary. OMOP CDM has some standard vicabularies that are the ones commonly used. In general this vocabulary will be different to the source one (originl of your data).\nWe call mapping to the process to convert a source data (original format of the data) that can be in many different formats to the OMOP CDM. In our team Antonella and Teen are the ones in charge of the mapping process.\nAthena contains the last version of the vocabularies and it is used to track its changes. You can use athena to search for OMOP concepts and see how they are related.",
    "crumbs": [
      "Onboarding",
      "OMOP the basis"
    ]
  },
  {
    "objectID": "onboarding/omop.html#final-remark",
    "href": "onboarding/omop.html#final-remark",
    "title": "OMOP the basis",
    "section": "",
    "text": "This was a very general introduction to the OMOP CDM as there are many resources out there that can help you to familiarise with OMOP. We recomend to take a look to The Book of OHDSI and the recommended courses from Ehden Academy, but the best way to learn about OMOP is to do an study with one of our OMOP instances, learning by doing :).",
    "crumbs": [
      "Onboarding",
      "OMOP the basis"
    ]
  },
  {
    "objectID": "onboarding/studies_and_authorship.html",
    "href": "onboarding/studies_and_authorship.html",
    "title": "Authorship and Conference attendance",
    "section": "",
    "text": "In our sub team we work on lots of things from running studies, writing code for studies and developing packages that others use for studies. In other teams, data scientists can sometimes be overlooked in authorship for papers however this is something we don’t want to happen in our team. Therefore we have created a guidance on authorship when we work within the team or with external collaborators. We are currently finalizing this document and in general terms, and unless previously agreed otherwise in writing, these principles will be used to decide on authorship and authorship positions:\n1) No external collaborators or authors\n\nEveryone named in the study protocol or analysis plan will in principle be eligible for authorship.\nPeople not mentioned in the protocol/analysis plan can be invited to participate. The study lead/co-lead will be responsible for the decision. This decision will be made in line and must be fully compliant with the ICMJE “Recommendations for the Conduct, Reporting, Editing, and Publication of Scholarly work in Medical Journals”.\nThe Senior study lead/s will have the right to name the first and last authors, with these being themselves or another member of the team\nIn cases with more than one study lead, joint last or corresponding author positions will be used to recognise joint senior leadership roles\nThe Junior study lead/s will be named as first author/joint first author in the resulting abstract or manuscript\nAll other involved staff mentioned in the study protocol/analysis plan will be offered an authorship position after the (two) first and before the (two) last authors\nAll people involved in the management, access, or curation of the data used for the study must be mentioned in the study protocol/analysis plan, and offered authorship\n\n2) External collaborators or authors\n\nAll the principles above will apply. Additional principles will be used to establish authorship for manuscripts co-authored by external staff:\nWhere external collaborators are mentioned and documented as co-authors or researchers in the study protocol/analysis plan, all these people should be invited to co-author scientific outputs as early as possible in the process, and before writing of the output/s\nWhere external collaborators are not mentioned and documented as co-authors or researchers in the study protocol/analysis plan, only the Senior study lead/s will be responsible for inviting them to be co-authors in scientific outputs. In this case, all co-authors should be notified as early as possible in the process, and before writing of the output/s\nTo qualify for co-authorship, external researchers will have to confirm their compliance with the ICMJE “Recommendations for the Conduct, Reporting, Editing, and Publication of Scholarly work in Medical Journals”.\nManuscripts including external as co-authors will require that all external co-authors report any (Conflict of interest) CoIs as early as possible. If they fail to do this by the planned time of submission, they will be excluded as co-authors. The Senior Study lead will be responsible for handling these discussions and/or exclusions (where applicable)\nWhere external co-authors work for a regulatory agency (e.g. EMA, MHRA, NICE) or a similar stakeholder, they should be consulted about their internal authorship and CoI policies as early as possible, as to assess if these are compatible with ours.\n\nMain take home: agree the authorship and know the conflict of interest policies before you start any study it will save a lot of hassle or unexpected conflict. Please discuss with your line manager about your contributions to your current studies and publications.\n\n\n\n\nFunding for Attendance: Our internal rule is that everyone in the team should be funded to attend at least one conference per year. We do not have any bespoke funding for this, but we will try to achieve this, and your line manager/main supervisor is responsible for securing this funding for you. You will only be eligible for this if you have a poster/abstract/talk to present at the target conference, so please make sure you submit something of quality to secure this.\nMore than 1 conference per year?: some people might be attending more than 1 conference per year. The group rule is that if this is your idea you need to find your own funding or have it ready before you submit your abstract.\nVisa Arrangements: Ensure that your visa is in order before proceeding with the registration process/travel bookings. We cannot help you much with this, so please make sure you do this early.\nTravel expenses and bookings: You can ask Mahki and her team to book your flights and travel for conference attendance and/or project meetings abroad.If you leave it to the last minute it will be up to you to sort flights/accommodation yourself and claim the costs back (which we do not recommend as claims can take many months). For other expenses that occur on your trip you can claim these back (within reason). Remember that university rules state that you must claim any expenses back via the e-expense app/website (SAP Concur) within 3 months from the purchase/expense date. For information about the expenses system see here",
    "crumbs": [
      "Onboarding",
      "Authorship and Conference attendance"
    ]
  },
  {
    "objectID": "onboarding/studies_and_authorship.html#authorship",
    "href": "onboarding/studies_and_authorship.html#authorship",
    "title": "Authorship and Conference attendance",
    "section": "",
    "text": "In our sub team we work on lots of things from running studies, writing code for studies and developing packages that others use for studies. In other teams, data scientists can sometimes be overlooked in authorship for papers however this is something we don’t want to happen in our team. Therefore we have created a guidance on authorship when we work within the team or with external collaborators. We are currently finalizing this document and in general terms, and unless previously agreed otherwise in writing, these principles will be used to decide on authorship and authorship positions:\n1) No external collaborators or authors\n\nEveryone named in the study protocol or analysis plan will in principle be eligible for authorship.\nPeople not mentioned in the protocol/analysis plan can be invited to participate. The study lead/co-lead will be responsible for the decision. This decision will be made in line and must be fully compliant with the ICMJE “Recommendations for the Conduct, Reporting, Editing, and Publication of Scholarly work in Medical Journals”.\nThe Senior study lead/s will have the right to name the first and last authors, with these being themselves or another member of the team\nIn cases with more than one study lead, joint last or corresponding author positions will be used to recognise joint senior leadership roles\nThe Junior study lead/s will be named as first author/joint first author in the resulting abstract or manuscript\nAll other involved staff mentioned in the study protocol/analysis plan will be offered an authorship position after the (two) first and before the (two) last authors\nAll people involved in the management, access, or curation of the data used for the study must be mentioned in the study protocol/analysis plan, and offered authorship\n\n2) External collaborators or authors\n\nAll the principles above will apply. Additional principles will be used to establish authorship for manuscripts co-authored by external staff:\nWhere external collaborators are mentioned and documented as co-authors or researchers in the study protocol/analysis plan, all these people should be invited to co-author scientific outputs as early as possible in the process, and before writing of the output/s\nWhere external collaborators are not mentioned and documented as co-authors or researchers in the study protocol/analysis plan, only the Senior study lead/s will be responsible for inviting them to be co-authors in scientific outputs. In this case, all co-authors should be notified as early as possible in the process, and before writing of the output/s\nTo qualify for co-authorship, external researchers will have to confirm their compliance with the ICMJE “Recommendations for the Conduct, Reporting, Editing, and Publication of Scholarly work in Medical Journals”.\nManuscripts including external as co-authors will require that all external co-authors report any (Conflict of interest) CoIs as early as possible. If they fail to do this by the planned time of submission, they will be excluded as co-authors. The Senior Study lead will be responsible for handling these discussions and/or exclusions (where applicable)\nWhere external co-authors work for a regulatory agency (e.g. EMA, MHRA, NICE) or a similar stakeholder, they should be consulted about their internal authorship and CoI policies as early as possible, as to assess if these are compatible with ours.\n\nMain take home: agree the authorship and know the conflict of interest policies before you start any study it will save a lot of hassle or unexpected conflict. Please discuss with your line manager about your contributions to your current studies and publications.",
    "crumbs": [
      "Onboarding",
      "Authorship and Conference attendance"
    ]
  },
  {
    "objectID": "onboarding/studies_and_authorship.html#conference-attendance",
    "href": "onboarding/studies_and_authorship.html#conference-attendance",
    "title": "Authorship and Conference attendance",
    "section": "",
    "text": "Funding for Attendance: Our internal rule is that everyone in the team should be funded to attend at least one conference per year. We do not have any bespoke funding for this, but we will try to achieve this, and your line manager/main supervisor is responsible for securing this funding for you. You will only be eligible for this if you have a poster/abstract/talk to present at the target conference, so please make sure you submit something of quality to secure this.\nMore than 1 conference per year?: some people might be attending more than 1 conference per year. The group rule is that if this is your idea you need to find your own funding or have it ready before you submit your abstract.\nVisa Arrangements: Ensure that your visa is in order before proceeding with the registration process/travel bookings. We cannot help you much with this, so please make sure you do this early.\nTravel expenses and bookings: You can ask Mahki and her team to book your flights and travel for conference attendance and/or project meetings abroad.If you leave it to the last minute it will be up to you to sort flights/accommodation yourself and claim the costs back (which we do not recommend as claims can take many months). For other expenses that occur on your trip you can claim these back (within reason). Remember that university rules state that you must claim any expenses back via the e-expense app/website (SAP Concur) within 3 months from the purchase/expense date. For information about the expenses system see here",
    "crumbs": [
      "Onboarding",
      "Authorship and Conference attendance"
    ]
  },
  {
    "objectID": "onboarding/basics.html",
    "href": "onboarding/basics.html",
    "title": "Getting started",
    "section": "",
    "text": "Although our expertise in the different domains is going to differ, everyone in OxInfer needs at least a basic understanding on the programming we do to develop study code and create R packages, our development workflow for doing this as a team, and core epidemiological concepts that underpin what we do. Below are various links to help you get started, but remember that many of the answers to the specific questions you might have will be in the heads of other team members so don’t hesitate to ask questions!\n\n\nR is the main language we use for both writing the code for studies and creating software (R packages we distribute via cran).\n\nR for data science https://r4ds.had.co.nz/\nAdvanced R https://adv-r.hadley.nz/\nR packages https://r-pkgs.org/\n\nWhen creating R packages, we largely follow the tidyverse style and their design guide provides a nice overview of their approach.\n\nTidyverse deign guide https://design.tidyverse.org/\n\nAlthough we rarely write SQL directly, often our R code gets translated into SQL when we’re working with a database (via the dbplyr package https://dbplyr.tidyverse.org/). So its also important to know the basics of SQL. There are a lot of resources online for learning SQL, but some that are nice to get started are (there is a lot of overlap between these so going through one of them should give you most of the basics):\n\nSQLBolt https://sqlbolt.com/\nMode SQL tutorial https://mode.com/sql-tutorial\nSQLZoo tutorial https://www.sqlzoo.net/wiki/SQL_Tutorial\n\n\n\n\nWe do our development of study code and software on github. In fact, even this book has it’s repo there https://github.com/oxford-pharmacoepi/OxinferOnboarding To understand why and how we use github see the below links\n\nhttps://medium.com/nyc-planning-digital/git-what-extolling-githubs-virtues-to-non-coders-6cc11f1a5fd2\nhttps://docs.github.com/en/get-started/using-github/github-flow\nhttps://happygitwithr.com/rstudio-git-github\n\n\n\n\n\nAs you’ll see in the next chapter, our work focuses on epidemiology with real world data that has been transformed into a common data model. Before getting into the weeds of this, in is important to know more generally what the field of epidemiology is all about:\n\n\nWhat is epidemiology?\nhttps://epirhandbook.com/",
    "crumbs": [
      "Onboarding",
      "Getting started"
    ]
  },
  {
    "objectID": "onboarding/basics.html#programming",
    "href": "onboarding/basics.html#programming",
    "title": "Getting started",
    "section": "",
    "text": "R is the main language we use for both writing the code for studies and creating software (R packages we distribute via cran).\n\nR for data science https://r4ds.had.co.nz/\nAdvanced R https://adv-r.hadley.nz/\nR packages https://r-pkgs.org/\n\nWhen creating R packages, we largely follow the tidyverse style and their design guide provides a nice overview of their approach.\n\nTidyverse deign guide https://design.tidyverse.org/\n\nAlthough we rarely write SQL directly, often our R code gets translated into SQL when we’re working with a database (via the dbplyr package https://dbplyr.tidyverse.org/). So its also important to know the basics of SQL. There are a lot of resources online for learning SQL, but some that are nice to get started are (there is a lot of overlap between these so going through one of them should give you most of the basics):\n\nSQLBolt https://sqlbolt.com/\nMode SQL tutorial https://mode.com/sql-tutorial\nSQLZoo tutorial https://www.sqlzoo.net/wiki/SQL_Tutorial",
    "crumbs": [
      "Onboarding",
      "Getting started"
    ]
  },
  {
    "objectID": "onboarding/basics.html#development-workflow",
    "href": "onboarding/basics.html#development-workflow",
    "title": "Getting started",
    "section": "",
    "text": "We do our development of study code and software on github. In fact, even this book has it’s repo there https://github.com/oxford-pharmacoepi/OxinferOnboarding To understand why and how we use github see the below links\n\nhttps://medium.com/nyc-planning-digital/git-what-extolling-githubs-virtues-to-non-coders-6cc11f1a5fd2\nhttps://docs.github.com/en/get-started/using-github/github-flow\nhttps://happygitwithr.com/rstudio-git-github",
    "crumbs": [
      "Onboarding",
      "Getting started"
    ]
  },
  {
    "objectID": "onboarding/basics.html#epidemiology",
    "href": "onboarding/basics.html#epidemiology",
    "title": "Getting started",
    "section": "",
    "text": "As you’ll see in the next chapter, our work focuses on epidemiology with real world data that has been transformed into a common data model. Before getting into the weeds of this, in is important to know more generally what the field of epidemiology is all about:\n\n\nWhat is epidemiology?\nhttps://epirhandbook.com/",
    "crumbs": [
      "Onboarding",
      "Getting started"
    ]
  },
  {
    "objectID": "the_team.html",
    "href": "the_team.html",
    "title": "Oxinfer",
    "section": "",
    "text": "Oxford distributed analytics for network research group (oxinfer) is a research group at the University of Oxford with three key purposes:\n\n\n\nOxinfer is part of the wider Health Data Sciences section (lead by Prof. Daniel Prieto-Alhambra)\nWelcome to the team! Below is some useful information and links about getting help with certain aspects of your role. You can check out our website where you can put names to faces [here].\nhttps://www.ndorms.ox.ac.uk/team/edward-burn\n\n\n\nOur Team Dec 23\n\n\n\n\n\nscope of the team … TBC\n\n\n\n\nmembers"
  },
  {
    "objectID": "the_team.html#the-oxinfer-team",
    "href": "the_team.html#the-oxinfer-team",
    "title": "Oxinfer",
    "section": "",
    "text": "scope of the team … TBC"
  },
  {
    "objectID": "the_team.html#members",
    "href": "the_team.html#members",
    "title": "Oxinfer",
    "section": "",
    "text": "members"
  }
]